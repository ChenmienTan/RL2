data:
  train_data_path: data/orz.json
  test_data_path: data/olympiadbench.json
  max_prompt_length: 2048
  batch_size: 128
  
actor:
  model_name: Qwen/Qwen2.5-7B
  gradient_checkpointing: true
  sp_size: 1
  max_length_per_device: 16384
  clip: 0.2
  update_per_rollout: 1
  lr: 1e-6
  weight_decay: 1e-2
  max_grad_norm: 1.0
  offload_model: true
  offload_optimizer: true

  rollout:
    tp_size: 1
    gpu_memory_utilization: 0.8
    rollout_per_prompt: 64
    train_temperature: 1.0
    test_temperature: 0.0
    max_response_length: 8192
    reward_fn_path: rewards/math.py

  kl:
    coef: 0.0
    type: null # `reward` or `loss`
    estimator: null # `k1`, `k2` or `k3`. See http://joschu.net/blog/kl-approx.html.
    # These two hyper-paratemers are orthogonal, so 
    # there are 6 possible combinations.
    # Two typical choices are
    #   - OpenAI PPO: type=reward, estimator=k1
    #   - DeepSeek GRPO: type=loss, estimator=k3

critic:
  model_name: Qwen/Qwen2.5-7B
  gradient_checkpointing: true
  sp_size: 1
  max_length_per_device: 16384
  clip: 0.5
  update_per_rollout: 4
  lr: 5e-6
  weight_decay: 1e-2
  max_grad_norm: 1.0
  offload_model: true
  offload_optimizer: true

adv:
  estimator: reinforce
  gamma: 1.0
  lamda: 1.0
  norm_var: false
  
trainer:
  project: OpenReasonerZero
  experiment_name: qwen2.5-7b
  n_epochs: 1
  test_freq: 8
  save_freq: 32
  save_dir: ckpts
  disable_wandb: false
  