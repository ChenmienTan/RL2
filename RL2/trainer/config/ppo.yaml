data:
  train_data_path: null
  test_data_path: null
  prompts_per_rollout: null
  responses_per_prompt: null
  
actor:
  model_name: null
  ref_model_name: ${actor.model_name}
  optimizer_dir: null
  gradient_checkpointing: true
  sp_size: 1
  max_length_per_device: null
  clip: 0.2
  update_per_rollout: 1
  lr: 1e-6
  weight_decay: 1e-2
  max_grad_norm: 1.0
  freeze_steps: 0
  offload_model: true
  offload_optimizer: true
  save_dir: ckpts/${trainer.experiment_name}/actor
  save_freq: null
  save_optimizer: true

  rollout:
    tp_size: 1
    gpu_memory_utilization: 0.5
    train_temperature: 1.0
    test_temperature: 0.0
    n_turns: 1
    max_response_length: null
    env_path: null
    group_filtering:
      lower: 0.0
      upper: 1.0

  kl:
    coef: 0.0
    type: null # `reward` or `loss`
    reward_estimator: k1
    loss_estimator: k2
    # `k1`, `k2` or `k3`. See http://joschu.net/blog/kl-approx.html.

  entropy:
    coef: 1e-4

critic:
  model_name: ${actor.model_name}
  optimizer_dir: null
  gradient_checkpointing: true
  sp_size: 1
  max_length_per_device: ${actor.max_length_per_device}
  clip: 0.5
  update_per_rollout: 12
  lr: 5e-6
  weight_decay: 1e-2
  max_grad_norm: 1.0
  offload_model: true
  offload_optimizer: true
  save_dir: ckpts/${trainer.experiment_name}/critic
  save_freq: ${actor.save_freq}
  save_optimizer: ${actor.save_optimizer}

adv:
  estimator: null # `reinforce` or `gae`
  gamma: 1.0
  lamda: 1.0
  norm_var: false
  
trainer:
  project: null
  experiment_name: null
  n_epochs: 1
  test_freq: null
  disable_wandb: false
  