train_data:
  path: null
  prompts_per_rollout: null
  responses_per_prompt: null

test_data:
  path: null
  response_per_prompt: 1
  
actor:
  model_name: null
  use_liger_kernel: false
  gradient_checkpointing: true
  ddp_size: 1
  tp_size: 1
  sp_size: 1
  max_length_per_device: null
  max_inference_length_per_device: ${actor.max_length_per_device}
  temperature: ${rollout.train_sampling_params.temperature}
  update_per_rollout: 1
  clip: 0.2
  avg_level: token
  lr: 1e-6
  weight_decay: 1e-2
  max_grad_norm: 1.0
  tis_coef: 0.0
  scheduler: constant
  warmup_ratio: 0.1
  freeze_steps: 0
  offload_model: true
  offload_optimizer: true

  kl:
    coef: 0.0
    type: null # `reward` or `loss`
    reward_estimator: k1
    loss_estimator: k2
    # `k1`, `k2` or `k3`. See http://joschu.net/blog/kl-approx.html.

  entropy:
    coef: 0.0

rollout:
  model_name: ${actor.model_name}
  dtype: bfloat16
  tp_size: 1
  gpu_memory_utilization: 0.7
  responses_per_prompt: ${train_data.responses_per_prompt}
  train_sampling_params:
    temperature: 1.0
    max_new_tokens: null
  test_sampling_params:
    temperature: 0.0
    max_new_tokens: ${rollout.train_sampling_params.max_new_tokens}
  max_turns: 1
  env_path: null
  dynamic_filtering: true
  use_gem_env: false  # Set to true to enable GEM environment integration
  
  # Optional GEM environment configuration - only used when rollout.use_gem_env = true
  gem_env:
    env_id: "rg:leg_counting"  # GEM environment ID (e.g., "math:GSM8K", "code:PrimeIntellect15k")
    wrappers: "concat_chat"    # GEM environment wrappers
    num_env: 2                 # Number of parallel environments
    async_env: true            # Use async vectorization for better performance
    seed: 233                  # Random seed for environment
    
    # Prompt and response handling
    prompt_template: "qwen3_game"  # Template for formatting observations ("qwen3_game", "no", "qwen3_general", "code")
    apply_chat_template: false     # Whether to apply tokenizer chat template
    max_model_len: 12800          # Maximum model context length
    
    # Episode collection settings
    rollout_batch_size: 128       # Number of transitions to collect per rollout
    keep_generation_failed: true  # Keep episodes where generation failed
    
    # Reward computation
    gamma: 1.0                    # Discount factor for computing returns
    
    # Logging and debugging
    dump_experience_every: -1     # Dump experience data every N steps (-1 to disable)

ref_actor:
  model_name: ${actor.model_name}
  use_liger_kernel: ${actor.use_liger_kernel}
  ddp_size: ${actor.ddp_size}
  tp_size: ${actor.tp_size}
  sp_size: ${actor.sp_size}
  max_inference_length_per_device: ${actor.max_length_per_device}
  temperature: ${rollout.train_sampling_params.temperature}
  offload_model: ${actor.offload_model}

critic:
  model_name: ${actor.model_name}
  gradient_checkpointing: ${actor.gradient_checkpointing}
  ddp_size: ${actor.ddp_size}
  tp_size: ${actor.tp_size}
  sp_size: ${actor.sp_size}
  max_length_per_device: ${actor.max_length_per_device}
  max_inference_length_per_device: ${critic.max_length_per_device}
  update_per_rollout: 12
  clip: 0.5
  avg_level: ${actor.avg_level}
  lr: 5e-6
  weight_decay: ${actor.weight_decay}
  max_grad_norm: ${actor.max_grad_norm}
  scheduler: ${actor.scheduler}
  warmup_ratio: ${actor.warmup_ratio}
  offload_model: ${actor.offload_model}
  offload_optimizer: ${actor.offload_optimizer}

adv:
  estimator: reinforce # `reinforce` or `gae`
  gamma: 1.0
  lamda: 1.0
  global_norm: false
  norm_var: false
  
trainer:
  project: null
  experiment_name: null
  load_ckpt_from: null
  n_epochs: 1
  test_freq: null
  save_dir: ckpts/${trainer.experiment_name}
  save_freq: null
  use_wandb: true
  