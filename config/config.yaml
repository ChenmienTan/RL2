data:
  train_data_path: null
  test_data_path: null
  prompts_per_rollout: null
  responses_per_prompt: null
  
actor:
  model_name: null
  gradient_checkpointing: true
  sp_size: 1
  max_length_per_device: null
  clip: 0.2
  update_per_rollout: 1
  lr: 1e-6
  weight_decay: 1e-2
  max_grad_norm: 1.0
  offload_model: false
  offload_optimizer: false
  save_dir: ckpts/${trainer.experiment_name}
  save_freq: -1

  rollout:
    tp_size: 1
    gpu_memory_utilization: 0.8
    train_temperature: 1.0
    test_temperature: 0.0
    max_response_length: null
    reward_fn_path: null

  kl:
    coef: 0.0
    type: null # `reward` or `loss`
    estimator: null # `k1`, `k2` or `k3`. See http://joschu.net/blog/kl-approx.html.
    # These two hyper-paratemers are orthogonal, so 
    # there are 6 possible combinations.
    # Two typical choices are
    #   - OpenAI PPO: type=reward, estimator=k1
    #   - DeepSeek GRPO: type=loss, estimator=k3

critic:
  model_name: ${actor.model_name}
  gradient_checkpointing: true
  sp_size: 1
  max_length_per_device: ${actor.max_length_per_device}
  clip: 0.5
  update_per_rollout: 1
  lr: 1e-5
  weight_decay: 1e-2
  max_grad_norm: 1.0
  offload_model: false
  offload_optimizer: false
  save_dir: ${actor.save_dir}
  save_freq: ${actor.save_freq}

adv:
  estimator: null # `reinforce` or `gae`
  gamma: 1.0
  lamda: 1.0
  norm_var: false
  
trainer:
  project: null
  experiment_name: null
  n_epochs: 1
  test_freq: null
  disable_wandb: false
  